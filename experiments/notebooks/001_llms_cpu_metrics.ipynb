{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b8f14d6",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8356828d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, psutil, torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from typing import Dict, Any\n",
    "from threading import Thread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ceeba97",
   "metadata": {},
   "source": [
    "## Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43457538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura o número de threads\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"20\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84e47401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura o prompt\n",
    "\n",
    "PROMPT = (\n",
    "    \"Liste e descreva brevemente as principais vantagens e desvantagens \"\n",
    "    \"de usar arquitetura de microsserviços em aplicações nativas na nuvem.\"\n",
    ")\n",
    "\n",
    "MAX_NEW_TOKENS  = 250\n",
    "NUM_REPLICAS    = 5    # quantidade de repetições do modelo\n",
    "SAMPLE_INTERVAL = 0.1  # segundos entre amostras de CPU/RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "676467dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura o modelo\n",
    "\n",
    "MODEL_NAMES = [\n",
    "    # \"meta-llama/Llama-2-7b-hf\",\n",
    "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e815a9c",
   "metadata": {},
   "source": [
    "## Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73bd0150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name: str, sample_interval: float = 0.05) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Carrega tokenizer+modelo em CPU e retorna métricas de:\n",
    "      - tempo de carregamento\n",
    "      - pico de RAM (MB) e CPU (%) durante o load\n",
    "    \"\"\"\n",
    "    proc = psutil.Process()\n",
    "    \n",
    "    # Limpa objetos anteriores\n",
    "    proc.cpu_percent(None)\n",
    "    \n",
    "    t0 = time.perf_counter()\n",
    "    mem0 = proc.memory_info().rss / 1024**2\n",
    "\n",
    "    # carrega\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, force_download=True)\n",
    "    model     = AutoModelForCausalLM.from_pretrained(model_name, force_download=True ,device_map=\"cpu\")\n",
    "    model.eval()\n",
    "\n",
    "    elapsed = time.perf_counter() - t0\n",
    "    # amostra por um curto instante após o load para capturar picos\n",
    "    samples = []\n",
    "    for _ in range(int(0.5 / sample_interval)):  # 0.5 s de amostragem pós-load\n",
    "        cpu = proc.cpu_percent(None)\n",
    "        mem = proc.memory_info().rss / 1024**2\n",
    "        samples.append((cpu, mem))\n",
    "        time.sleep(sample_interval)\n",
    "\n",
    "    cpus, mems = zip(*samples)\n",
    "    return {\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"model\": model,\n",
    "        \"load_time_s\": elapsed,\n",
    "        \"load_peak_cpu_pct\": max(cpus),\n",
    "        \"load_peak_ram_mb\": max(mems) - mem0,\n",
    "    }\n",
    "\n",
    "\n",
    "def run_test(model, tokenizer, input_ids, attention_mask, MAX_NEW_TOKENS, SAMPLE_INTERVAL) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Executa uma única inferência, coletando métricas de tempo e uso de recursos.\n",
    "    Faz warm-up rápido antes da medição.\n",
    "    \"\"\"\n",
    "    # Warm-up\n",
    "    with torch.no_grad():\n",
    "        _ = model.generate(input_ids, max_new_tokens=24)\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "    # Métricas antes da geração\n",
    "    proc = psutil.Process()\n",
    "    proc.cpu_percent(None)\n",
    "    mem0 = proc.memory_info().rss / 1024**2\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    results = {}\n",
    "    def gen():\n",
    "        with torch.no_grad():\n",
    "            results[\"outputs\"] = model.generate(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_new_tokens=MAX_NEW_TOKENS,\n",
    "                do_sample=False,\n",
    "                eos_token_id=None,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "    \n",
    "    thread = Thread(target=gen, daemon=True)\n",
    "    thread.start()\n",
    "    \n",
    "    samples = []\n",
    "    while thread.is_alive():\n",
    "        elapsed = time.perf_counter() - t0\n",
    "        cpu = proc.cpu_percent(interval=SAMPLE_INTERVAL)\n",
    "        mem = proc.memory_info().rss / 1024**2\n",
    "        samples.append({\"time\": elapsed, \"cpu\": cpu, \"mem\": mem})\n",
    "        time.sleep(SAMPLE_INTERVAL)\n",
    "    thread.join()\n",
    "\n",
    "    # métricas finais\n",
    "    t1 = time.perf_counter()\n",
    "    \n",
    "    outputs = results[\"outputs\"]\n",
    "    total_tokens = outputs.shape[-1] - input_ids.shape[-1]\n",
    "    elapsed = t1 - t0\n",
    "    \n",
    "    cpus = [s[\"cpu\"] for s in samples]\n",
    "    mems = [s[\"mem\"] for s in samples]\n",
    "\n",
    "    return {\n",
    "        \"tokens\": total_tokens,\n",
    "        \"time_s\": elapsed,\n",
    "        \"latency_ms_per_token\": elapsed / total_tokens * 1e3,\n",
    "        \"throughput_tps\": total_tokens / elapsed,\n",
    "        \"inf_peak_cpu_pct\": max(cpus),\n",
    "        \"inf_peak_ram_mb\": max(mems) - mem0,\n",
    "        \"samples\": samples,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ee4d7c",
   "metadata": {},
   "source": [
    "## Generate Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12a6576d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Modelo: TinyLlama/TinyLlama-1.1B-Chat-v1.0 ====\n",
      "Carregando modelo...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b4103a4a90f4948b745cb0c9bbe82f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93df8779a2424f67915a6691806848a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "099a1ece55154c49ad6f93ddcef46eab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a854167f38f4d7ab4432946033b190b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd3d89b430c746a5b9b9b329b27411aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b90d518551d2477dae3835951dc74eab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c46d64ba42847208e582b3074a4b5a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c3ae37747e455098ac8712d5844ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5d72d2e2ecb485585523b9c418ce9d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f25eab4ff9452e81e535aa9920844a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizando prompt...\n",
      "Executando teste...\n",
      " Run 0/4...\n",
      " Run 1/4...\n",
      " Run 2/4...\n",
      " Run 3/4...\n",
      " Run 4/4...\n"
     ]
    }
   ],
   "source": [
    "# Loop para medir uso de CPU e RAM\n",
    "\n",
    "load_results = []\n",
    "inf_results  = []\n",
    "\n",
    "for model_name in MODEL_NAMES:\n",
    "    print(f\"\\n==== Modelo: {model_name} ====\")\n",
    "    \n",
    "    # Carrega o modelo e o tokenizador\n",
    "    print(\"Carregando modelo...\")\n",
    "    loaded_model = load_model(model_name, sample_interval=SAMPLE_INTERVAL)\n",
    "    tokenizer, model = loaded_model[\"tokenizer\"], loaded_model[\"model\"]\n",
    "    \n",
    "    # Coleta as métricas de carregamento\n",
    "    load_results.append({\n",
    "        \"model\": model_name,\n",
    "        \"load_time_s\": loaded_model[\"load_time_s\"],\n",
    "        \"load_peak_cpu_pct\": loaded_model[\"load_peak_cpu_pct\"],\n",
    "        \"load_peak_ram_mb\": loaded_model[\"load_peak_ram_mb\"],\n",
    "    })\n",
    "\n",
    "    # Tokeniza o prompt\n",
    "    print(\"Tokenizando prompt...\")\n",
    "    inputs         = tokenizer(PROMPT, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids      = inputs.input_ids\n",
    "    attention_mask = inputs.attention_mask\n",
    "\n",
    "    # Gera as métricas para o modelo\n",
    "    print(\"Executando teste...\")\n",
    "    for run in range(NUM_REPLICAS):\n",
    "        print(f\" Run {run}/{NUM_REPLICAS-1}...\")\n",
    "        res = run_test(model, tokenizer, input_ids, attention_mask, MAX_NEW_TOKENS, SAMPLE_INTERVAL)\n",
    "        res.update({\"model\": model_name, \"run\": run})\n",
    "        inf_results.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee94432e",
   "metadata": {},
   "source": [
    "## Analyse Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "274ac0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforma os resultados em um DataFrame\n",
    "\n",
    "df_load = pd.DataFrame(load_results)\n",
    "df_inf  = pd.DataFrame(inf_results)\n",
    "\n",
    "summary_load = df_load.groupby(\"model\").agg(\n",
    "    runs=(\"model\",\"count\"),\n",
    "    avg_load_time_s=(\"load_time_s\",\"mean\"),\n",
    "    std_load_time_s=(\"load_time_s\",\"std\"),\n",
    "    avg_peak_cpu_pct=(\"load_peak_cpu_pct\",\"mean\"),\n",
    "    avg_peak_ram_mb=(\"load_peak_ram_mb\",\"mean\"),\n",
    ").reset_index()\n",
    "\n",
    "summary_inf = df_inf.groupby(\"model\").agg(\n",
    "    runs=(\"run\",\"count\"),\n",
    "    avg_latency_ms_tok=(\"latency_ms_per_token\",\"mean\"),\n",
    "    std_latency_ms_tok=(\"latency_ms_per_token\",\"std\"),\n",
    "    avg_tps=(\"throughput_tps\",\"mean\"),\n",
    "    std_tps=(\"throughput_tps\",\"std\"),\n",
    "    avg_inf_peak_cpu=(\"inf_peak_cpu_pct\",\"mean\"),\n",
    "    avg_inf_peak_ram_mb=(\"inf_peak_ram_mb\",\"mean\"),\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546c431c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporta os resultados para CSV\n",
    "\n",
    "df_load.to_csv(\"../../data/raw/benchmarks/benchmark_carregamento.csv\", index=False)\n",
    "df_inf.to_csv(\"../../data/raw/benchmarks/benchmark_inferencia.csv\", index=False)\n",
    "summary_inf.to_csv(\"../../data/raw/benchmarks/benchmark_inferencia_resumo.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
