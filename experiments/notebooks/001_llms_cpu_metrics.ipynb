{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21a42d40",
   "metadata": {},
   "source": [
    "# Notebook Description\n",
    "\n",
    "- O objetivo desse notebook é testar o uso de recursos para carregamento e inferência em LLMs locais.\n",
    "- Busca-se coletar métricas de CPU e RAM para modelos de diferentes famílias (Llama, TinyLlama, Qwen, Gemma e Mistral).\n",
    "- Algumas versões das famílias e modelos são esperadas que retornem erro devido à limitação de RAM.\n",
    "- A máquina utizada para o teste possuim 32 de RAM e uma CPU Intel Core i5-13600k (14/20).\n",
    "- As métricas são salvas em arquivos CSV, possibilitando análise posterior.\n",
    "\n",
    "**Observações**\n",
    "- É coletado o uso absoluto do sistema, ou seja, não é retirado o que já estava em uso no sistema antes do teste\n",
    "- Há um overhead causado pelo ResourseMonitor (o que também ocorrerá na ferramenta final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8f14d6",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8356828d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, time, threading, psutil, torch, dotenv\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, logging\n",
    "from huggingface_hub.utils import disable_progress_bars\n",
    "from typing import List, Dict, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ceeba97",
   "metadata": {},
   "source": [
    "## Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43457538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura o ambiente\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"]      = \"12\" # Número de threads para OpenMP\n",
    "os.environ[\"MKL_NUM_THREADS\"]      = \"12\" # Número de threads para MKL\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"   # Garante que os processos não usem GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2db5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suprime verbose do Transformers, PyTorch e Hugging Face\n",
    "\n",
    "# Desabilita as barras de progresso do Hugging Face\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
    "\n",
    "disable_progress_bars()\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "logging.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b61876e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura o token do huggingface\n",
    "\n",
    "# Carrega as variáveis de ambiente do arquivo .env\n",
    "dotenv.load_dotenv(\"../../env/.env\")\n",
    "access_token = os.getenv(\"HF_TOKEN\")\n",
    "if not access_token:\n",
    "    raise RuntimeError(\"HF_TOKEN não definido!\")\n",
    "\n",
    "# Salva o token no cache do huggingface\n",
    "from huggingface_hub import HfFolder\n",
    "HfFolder.save_token(access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84e47401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura o prompt\n",
    "\n",
    "PROMPT = (\n",
    "    \"Liste e descreva brevemente as principais vantagens e desvantagens \"\n",
    "    \"de usar arquitetura de microsserviços em aplicações nativas na nuvem.\"\n",
    ")\n",
    "\n",
    "MAX_NEW_TOKENS  = 200  # Máximo de tokens a serem gerados\n",
    "NUM_REPLICAS    = 5    # Quantidade de repetições do modelo\n",
    "SAMPLE_INTERVAL = 0.1  # Segundos entre amostras de CPU/RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676467dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura o modelo\n",
    "# Modelos comentados causam a morte do kerel na máquina especificada\n",
    "\n",
    "MODEL_NAMES = [\n",
    "    \"meta-llama/Llama-3.2-1B\",            # Modelos Llama\n",
    "    # \"meta-llama/Llama-3.2-3B\",\n",
    "    \"meta-llama/Llama-3.1-8B\",\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    \"meta-llama/Llama-2-13b-hf\",\n",
    "    \"TinyLlama/TinyLlama_v1.1\",\n",
    "    \"Qwen/Qwen3-0.6B\",                    # Modelos Qwen\n",
    "    \"Qwen/Qwen3-1.7B\",\n",
    "    # \"Qwen/Qwen3-4B\",\n",
    "    \"Qwen/Qwen3-8B\",\n",
    "    \"Qwen/Qwen2.5-0.5B\",\n",
    "    # \"Qwen/Qwen2.5-3B\",\n",
    "    \"Qwen/Qwen2.5-7B\",\n",
    "    \"Qwen/Qwen2.5-14B\",\n",
    "    \"google/gemma-3-1b-it\",               # Modelos Gemma\n",
    "    # \"google/gemma-3-4b-it\",\n",
    "    \"google/gemma-3-12b-it\",\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.3\", # Modelos Mistral\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36667577",
   "metadata": {},
   "source": [
    "## Resource Monitoring Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6288b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResourceMonitor:\n",
    "    def __init__(self, interval: float = 0.1):\n",
    "        self.interval = interval\n",
    "        self._stop_event = threading.Event()\n",
    "        self._thread = threading.Thread(target=self._run, daemon=True)\n",
    "        self.samples: List[Dict[str, Any]] = []\n",
    "\n",
    "    def _run(self):\n",
    "        proc = psutil.Process()\n",
    "        while not self._stop_event.is_set():\n",
    "            # Por processo\n",
    "            cpu_proc = proc.cpu_percent(None)\n",
    "            mem_proc = proc.memory_info().rss / 1024**2\n",
    "            # Por sistema\n",
    "            cpu_sys = psutil.cpu_percent(None)\n",
    "            mem_sys = psutil.virtual_memory().used / 1024**2\n",
    "\n",
    "            timestamp = time.perf_counter()\n",
    "            self.samples.append({\n",
    "                \"time\": timestamp,\n",
    "                \"cpu_proc_pct\": cpu_proc,\n",
    "                \"mem_proc_mb\": mem_proc,\n",
    "                \"cpu_sys_pct\": cpu_sys,\n",
    "                \"mem_sys_mb\": mem_sys,\n",
    "            })\n",
    "            time.sleep(self.interval)\n",
    "\n",
    "    def start(self):\n",
    "        self.samples = []\n",
    "        self._stop_event.clear()\n",
    "        psutil.Process().cpu_percent(None)\n",
    "        psutil.cpu_percent(None)\n",
    "        self._thread = threading.Thread(target=self._run, daemon=True)\n",
    "        self._thread.start()\n",
    "\n",
    "    def stop(self):\n",
    "        self._stop_event.set()\n",
    "        self._thread.join()\n",
    "        return self.samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e815a9c",
   "metadata": {},
   "source": [
    "## Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73bd0150",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model(model_name: str, sample_interval: float = 0.05) -> Dict[str, Any]:    \n",
    "    # Inicia o monitoramento de recursos    \n",
    "    monitor = ResourceMonitor(interval=sample_interval)\n",
    "    t0 = time.perf_counter()\n",
    "    monitor.start()\n",
    "    \n",
    "    # Carrega tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        force_download=True,\n",
    "        token=access_token,\n",
    "    )\n",
    "    \n",
    "    # Carrega modelo\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        force_download=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        token=access_token,\n",
    "        device_map=\"cpu\"\n",
    "    )\n",
    "    \n",
    "    # Para o monitoramento\n",
    "    monitor.stop()\n",
    "    elapsed = time.perf_counter() - t0\n",
    "    \n",
    "    # Adiciona token de padding se não existir\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # Adiciona truncação se não existir\n",
    "    if tokenizer.model_max_length is None or tokenizer.model_max_length > 10000:\n",
    "        tokenizer.model_max_length = 512\n",
    "\n",
    "    # Coleta as amostras de uso de recursos\n",
    "    samples = monitor.samples\n",
    "    cpu_proc_max = max(s[\"cpu_proc_pct\"] for s in samples)\n",
    "    mem_proc_max = max(s[\"mem_proc_mb\"]  for s in samples)\n",
    "    cpu_sys_max  = max(s[\"cpu_sys_pct\"]  for s in samples)\n",
    "    mem_sys_max  = max(s[\"mem_sys_mb\"]   for s in samples)\n",
    "    \n",
    "    return {\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"model\": model,\n",
    "        \"load_time_s\": elapsed,\n",
    "        \"load_peak_cpu_proc_pct\": cpu_proc_max,\n",
    "        \"load_peak_ram_proc_mb\": mem_proc_max,\n",
    "        \"load_peak_cpu_sys_pct\": cpu_sys_max,\n",
    "        \"load_peak_ram_sys_mb\": mem_sys_max,\n",
    "        \"load_samples\": samples,\n",
    "    }\n",
    "\n",
    "\n",
    "def run_test(model, tokenizer, input_ids, attention_mask,\n",
    "             max_new_tokens: int, sample_interval: float) -> Dict[str, Any]:\n",
    "    # Warm-up\n",
    "    with torch.no_grad():\n",
    "        _ = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=24,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    # Inicia o monitoramento de recursos\n",
    "    monitor = ResourceMonitor(interval=sample_interval)\n",
    "    t0 = time.perf_counter()\n",
    "    monitor.start()\n",
    "\n",
    "    # Executa a inferência\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "    # Para o monitoramento\n",
    "    monitor.stop()\n",
    "    elapsed = time.perf_counter() - t0\n",
    "    \n",
    "    samples = monitor.samples\n",
    "    cpu_proc_max = max(s[\"cpu_proc_pct\"] for s in samples)\n",
    "    mem_proc_max = max(s[\"mem_proc_mb\"]  for s in samples)\n",
    "    cpu_sys_max  = max(s[\"cpu_sys_pct\"]  for s in samples)\n",
    "    mem_sys_max  = max(s[\"mem_sys_mb\"]   for s in samples)  \n",
    "\n",
    "    total_tokens = outputs.shape[-1] - input_ids.shape[-1]\n",
    "    return {\n",
    "        \"tokens\": total_tokens,\n",
    "        \"time_s\": elapsed,\n",
    "        \"latency_ms_per_token\": elapsed/total_tokens*1e3,\n",
    "        \"throughput_tps\": total_tokens/elapsed,\n",
    "        \"inf_peak_cpu_proc_pct\": cpu_proc_max,\n",
    "        \"inf_peak_ram_proc_mb\": mem_proc_max,\n",
    "        \"inf_peak_cpu_sys_pct\": cpu_sys_max,\n",
    "        \"inf_peak_ram_sys_mb\": mem_sys_max,\n",
    "        \"inf_samples\": samples,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ee4d7c",
   "metadata": {},
   "source": [
    "## Generate Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a6576d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Modelo: meta-llama/Llama-3.2-1B ====\n",
      "Carregando modelo...\n",
      "Tokenizando prompt...\n",
      "Executando teste...\n",
      " Run 0/4...\n",
      " Run 1/4...\n",
      " Run 2/4...\n",
      " Run 3/4...\n",
      " Run 4/4...\n",
      "Limpeza de memória concluída. Iniciando próximo modelo...\n",
      "\n",
      "\n",
      "==== Modelo: meta-llama/Llama-3.1-8B ====\n",
      "Carregando modelo...\n",
      "Erro ao carregar meta-llama/Llama-3.1-8B: [enforce fail at alloc_cpu.cpp:119] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 32121044992 bytes. Error code 12 (Cannot allocate memory)\n",
      "\n",
      "\n",
      "==== Modelo: meta-llama/Llama-2-7b-hf ====\n",
      "Carregando modelo...\n",
      "Erro ao carregar meta-llama/Llama-2-7b-hf: [enforce fail at alloc_cpu.cpp:119] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 26953662464 bytes. Error code 12 (Cannot allocate memory)\n",
      "\n",
      "\n",
      "==== Modelo: meta-llama/Llama-2-13b-hf ====\n",
      "Carregando modelo...\n",
      "Erro ao carregar meta-llama/Llama-2-13b-hf: [enforce fail at alloc_cpu.cpp:119] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 52063457280 bytes. Error code 12 (Cannot allocate memory)\n",
      "\n",
      "\n",
      "==== Modelo: TinyLlama/TinyLlama_v1.1 ====\n",
      "Carregando modelo...\n",
      "Tokenizando prompt...\n",
      "Executando teste...\n",
      " Run 0/4...\n",
      " Run 1/4...\n",
      " Run 2/4...\n",
      " Run 3/4...\n",
      " Run 4/4...\n",
      "Limpeza de memória concluída. Iniciando próximo modelo...\n",
      "\n",
      "\n",
      "==== Modelo: Qwen/Qwen3-0.6B ====\n",
      "Carregando modelo...\n",
      "Tokenizando prompt...\n",
      "Executando teste...\n",
      " Run 0/4...\n",
      " Run 1/4...\n",
      " Run 2/4...\n",
      " Run 3/4...\n",
      " Run 4/4...\n",
      "Limpeza de memória concluída. Iniciando próximo modelo...\n",
      "\n",
      "\n",
      "==== Modelo: Qwen/Qwen3-1.7B ====\n",
      "Carregando modelo...\n",
      "Tokenizando prompt...\n",
      "Executando teste...\n",
      " Run 0/4...\n",
      " Run 1/4...\n",
      " Run 2/4...\n",
      " Run 3/4...\n",
      " Run 4/4...\n",
      "Limpeza de memória concluída. Iniciando próximo modelo...\n",
      "\n",
      "\n",
      "==== Modelo: Qwen/Qwen3-8B ====\n",
      "Carregando modelo...\n",
      "Erro ao carregar Qwen/Qwen3-8B: [enforce fail at alloc_cpu.cpp:119] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 32762941440 bytes. Error code 12 (Cannot allocate memory)\n",
      "\n",
      "\n",
      "==== Modelo: Qwen/Qwen2.5-0.5B ====\n",
      "Carregando modelo...\n",
      "Tokenizando prompt...\n",
      "Executando teste...\n",
      " Run 0/4...\n",
      " Run 1/4...\n",
      " Run 2/4...\n",
      " Run 3/4...\n",
      " Run 4/4...\n",
      "Limpeza de memória concluída. Iniciando próximo modelo...\n",
      "\n",
      "\n",
      "==== Modelo: Qwen/Qwen2.5-7B ====\n",
      "Carregando modelo...\n",
      "Erro ao carregar Qwen/Qwen2.5-7B: [enforce fail at alloc_cpu.cpp:119] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 30462466048 bytes. Error code 12 (Cannot allocate memory)\n",
      "\n",
      "\n",
      "==== Modelo: Qwen/Qwen2.5-14B ====\n",
      "Carregando modelo...\n",
      "Erro ao carregar Qwen/Qwen2.5-14B: [enforce fail at alloc_cpu.cpp:119] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 59080134656 bytes. Error code 12 (Cannot allocate memory)\n",
      "\n",
      "\n",
      "==== Modelo: google/gemma-3-1b-it ====\n",
      "Carregando modelo...\n",
      "Tokenizando prompt...\n",
      "Executando teste...\n",
      " Run 0/4...\n",
      " Run 1/4...\n",
      " Run 2/4...\n",
      " Run 3/4...\n",
      " Run 4/4...\n",
      "Limpeza de memória concluída. Iniciando próximo modelo...\n",
      "\n",
      "\n",
      "==== Modelo: google/gemma-3-12b-it ====\n",
      "Carregando modelo...\n",
      "Erro ao carregar google/gemma-3-12b-it: [enforce fail at alloc_cpu.cpp:119] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 52776815040 bytes. Error code 12 (Cannot allocate memory)\n",
      "\n",
      "\n",
      "==== Modelo: mistralai/Mistral-7B-Instruct-v0.3 ====\n",
      "Carregando modelo...\n",
      "Erro ao carregar mistralai/Mistral-7B-Instruct-v0.3: [enforce fail at alloc_cpu.cpp:119] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 28992094208 bytes. Error code 12 (Cannot allocate memory)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop para medir uso de CPU e RAM\n",
    "# Erros ao carregar alguns modelos são esperados e não devem interromper o loop\n",
    "\n",
    "load_results = []\n",
    "inf_results  = []\n",
    "\n",
    "for model_name in MODEL_NAMES:\n",
    "    print(f\"\\n==== Modelo: {model_name} ====\")\n",
    "    \n",
    "    # Tenta carregar o modelo; se falhar, pula para o próximo\n",
    "    try:\n",
    "        print(\"Carregando modelo...\")\n",
    "        loaded = load_model(model_name, sample_interval=SAMPLE_INTERVAL)\n",
    "    except (OSError, RuntimeError) as e:\n",
    "        print(f\"Erro ao carregar {model_name}: {e}\\n\")\n",
    "        continue\n",
    "\n",
    "    # Carrega o tokenizer (model já carregado sem erros)\n",
    "    tokenizer, model = loaded[\"tokenizer\"], loaded[\"model\"]\n",
    "    \n",
    "    # Salva as métricas de carregamento\n",
    "    load_results.append({\n",
    "        \"model\"                  : model_name,\n",
    "        \"load_time_s\"            : loaded[\"load_time_s\"],\n",
    "        \"load_peak_cpu_proc_pct\" : loaded[\"load_peak_cpu_proc_pct\"],\n",
    "        \"load_peak_ram_proc_mb\"  : loaded[\"load_peak_ram_proc_mb\"],\n",
    "        \"load_peak_cpu_sys_pct\"  : loaded[\"load_peak_cpu_sys_pct\"],\n",
    "        \"load_peak_ram_sys_mb\"   : loaded[\"load_peak_ram_sys_mb\"],\n",
    "        \"load_samples\"           : loaded[\"load_samples\"],\n",
    "    })\n",
    "\n",
    "    # Tokeniza o prompt\n",
    "    print(\"Tokenizando prompt...\")\n",
    "    inputs         = tokenizer(PROMPT, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids      = inputs.input_ids\n",
    "    attention_mask = inputs.attention_mask\n",
    "\n",
    "    # Salva as métricas para cada modelo\n",
    "    print(\"Executando teste...\")\n",
    "    for run in range(NUM_REPLICAS):\n",
    "        print(f\" Run {run}/{NUM_REPLICAS-1}...\")\n",
    "        \n",
    "        try:\n",
    "            res = run_test(\n",
    "                model, tokenizer,\n",
    "                input_ids, attention_mask,\n",
    "                MAX_NEW_TOKENS, SAMPLE_INTERVAL\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Erro durante a inferência: {e}\")\n",
    "            continue\n",
    "        \n",
    "        res.update({\"model\": model_name, \"run\": run})\n",
    "        inf_results.append(res)\n",
    "    \n",
    "    # Limpeza antes do próximo modelo\n",
    "    del model, tokenizer, loaded\n",
    "    gc.collect()\n",
    "    print(\"Limpeza de memória concluída. Iniciando próximo modelo...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee94432e",
   "metadata": {},
   "source": [
    "## Analyse Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "274ac0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforma os resultados em um DataFrame\n",
    "\n",
    "df_load = pd.DataFrame(load_results)\n",
    "df_inf  = pd.DataFrame(inf_results)\n",
    "\n",
    "summary_load = (\n",
    "    df_load\n",
    "    .groupby(\"model\")\n",
    "    .agg(\n",
    "        runs                       = (\"model\",                  \"count\"),\n",
    "        avg_load_time_s            = (\"load_time_s\",            \"mean\"),\n",
    "        avg_load_peak_cpu_proc_pct = (\"load_peak_cpu_proc_pct\", \"mean\"),\n",
    "        avg_load_peak_ram_proc_mb  = (\"load_peak_ram_proc_mb\",  \"mean\"),\n",
    "        avg_load_peak_cpu_sys_pct  = (\"load_peak_cpu_sys_pct\",  \"mean\"),\n",
    "        avg_load_peak_ram_sys_mb   = (\"load_peak_ram_sys_mb\",   \"mean\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "summary_inf = (\n",
    "    df_inf\n",
    "    .groupby(\"model\")\n",
    "    .agg(\n",
    "        runs                      = (\"run\",                   \"count\"),\n",
    "        avg_latency_ms_per_token  = (\"latency_ms_per_token\",  \"mean\"),\n",
    "        std_latency_ms_per_token  = (\"latency_ms_per_token\",  \"std\"),\n",
    "        avg_throughput_tps        = (\"throughput_tps\",        \"mean\"),\n",
    "        std_throughput_tps        = (\"throughput_tps\",        \"std\"),\n",
    "        avg_inf_peak_cpu_proc_pct = (\"inf_peak_cpu_proc_pct\", \"mean\"),\n",
    "        std_inf_peak_cpu_proc_pct = (\"inf_peak_cpu_proc_pct\", \"std\"),\n",
    "        avg_inf_peak_ram_proc_mb  = (\"inf_peak_ram_proc_mb\",  \"mean\"),\n",
    "        std_inf_peak_ram_proc_mb  = (\"inf_peak_ram_proc_mb\",  \"std\"),\n",
    "        avg_inf_peak_cpu_sys_pct  = (\"inf_peak_cpu_sys_pct\",  \"mean\"),\n",
    "        std_inf_peak_cpu_sys_pct  = (\"inf_peak_cpu_sys_pct\",  \"std\"),\n",
    "        avg_inf_peak_ram_sys_mb   = (\"inf_peak_ram_sys_mb\",   \"mean\"),\n",
    "        std_inf_peak_ram_sys_mb   = (\"inf_peak_ram_sys_mb\",   \"std\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "546c431c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporta os resultados para CSV\n",
    "\n",
    "df_load.to_csv(\"../../data/raw/benchmarks/benchmark_carregamento.csv\", index=False)\n",
    "df_inf.to_csv(\"../../data/raw/benchmarks/benchmark_inferencia.csv\", index=False)\n",
    "summary_inf.to_csv(\"../../data/raw/benchmarks/benchmark_inferencia_resumo.csv\", index=False)\n",
    "summary_load.to_csv(\"../../data/raw/benchmarks/benchmark_carregamento_resumo.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
